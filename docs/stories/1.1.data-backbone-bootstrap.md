# Story 1.1: Data Backbone Bootstrap

- **Status:** Draft
- **Epic:** 1 – Data Backbone Foundations
- **Related PRD Sections:** `docs/prd/05-5-scope-phase-1-mvp.md`, `docs/prd/07-7-detailed-requirements.md`, `docs/prd/09-9-data-model-integrations.md`
- **Related Architecture Sections:** `docs/architecture/03-3-high-level-architecture-overview.md`, `docs/architecture/04-4-component-breakdown.md`, `docs/architecture/05-5-key-data-flows.md`

## Story Statement
As the TillLess platform operator, I want the Phase 1 canonical product registry schema live and a production-grade Checkers/Sixty60 ingestion loop writing fresh catalogue and price snapshots, so that downstream optimisation can rely on accurate data for the pilot geography.

## Acceptance Criteria
- [ ] Postgres instance (staging + prod) updated with migration `20241007_0001_cpr_phase1.sql`; new columns (`price_mode`, `price_per_uom`, loyalty/promo fields, store metadata) verified via introspection queries.
- [ ] Checkers/Sixty60 scraper worker (Playwright-based) runs on schedule (manual trigger acceptable for MVP) and publishes payloads through the ingestion queue into CPR tables.
- [ ] Ingestion writes populate `retailer_item` and `retailer_item_price` with ≥500 active SKUs covering target staple categories (milk, bread, rice, oil, detergents, nappies, chicken) for at least one Johannesburg store.
- [ ] `content_hash` prevents duplicate writes on consecutive runs; rerunning worker without catalogue changes results in ≤5% new rows.
- [ ] Monitoring emits job status (success/failure, duration, SKU count) to central log/metric sink with alert threshold for two consecutive failures.
- [ ] Runbook entry updated/created summarising how to trigger the Checkers ingestion and verify data freshness.

## Tasks & Subtasks
1. **Apply CPR Phase 1 Migration**
   - Run migration script against staging; capture before/after snapshot of schema.
   - Repeat for production (or note pending release) once staging signed off.
   - Document verification queries in `db/README.md` (append if needed).
2. **Implement Checkers Scraper Worker**
   - Bootstrap Playwright script per `docs/retailer-scraping-playbook.md` (Sixty60 GraphQL endpoints).
   - Add polite rate limiting, auth token management, and `content_hash` calculation.
   - Package worker for container execution (Dockerfile + environment variables).
3. **Set Up Ingestion Queue Path**
   - Provision RabbitMQ/SQS queue; define payload schema and retry policy.
   - Implement queue consumer that maps payloads to CPR schema (parsing promo, loyalty, pack data).
   - Write integration tests/assertions for critical fields before DB insert.
4. **Seed Canonical Data**
   - Run worker to collect initial dataset for a representative Gauteng store.
   - Validate ≥500 SKUs written with correct `price`, `loyalty_price`, `price_mode`, `availability`.
   - Spot-check 10 items manually against live site for accuracy.
5. **Observability & Runbook**
   - Emit logs/metrics to chosen sink (CloudWatch/Grafana/Stdout parsed) including SKU count and scrape timestamp.
   - Configure alert for two consecutive failures or zero SKU output.
   - Update/create ingestion runbook section referencing commands, expected output, troubleshooting.

## Implementation Notes for Dev Agent
- Prioritise alignment with CPR schema (`docs/canonical-product-registry.md`); pay attention to `price_mode` and `price_per_uom` handling for per-weight items.
- Authentication secrets for Sixty60 stored via secrets manager; ensure worker reads from env variables and never logs sensitive data.
- Queue payload should carry raw fields (`product_name_raw`, `promo_badge_raw`, etc.) so consumer can persist unchanged values for auditing.
- Use batching/bulk insert to minimise DB round trips while respecting transaction size limits.
- Coordinate with infrastructure story for scheduler; for this story, a manual trigger plus documented command is acceptable to satisfy acceptance criteria.

## Testing & Validation
- Automated: Unit tests for payload normalisation and DB insert mapping (mock Postgres). Integration test hitting staging DB to ensure schema compatibility.
- Manual: Trigger scraper twice; confirm second run outputs “no change” behaviour. Validate sample entries via SQL queries.
- Performance spot-check: Ensure end-to-end run completes within configured time window (<10 minutes for 500 SKUs).

## Dependencies / Sequencing Notes
- Relies on availability of Postgres instance and secrets store (credentials for Sixty60, database, queue).
- Downstream optimisation stories depend on this data being present; schedule this as the first development story of Epic 1.
- Future stories (1.2–1.5) will add remaining retailers, monitoring dashboards, and receipt feedback ingestion.

## QA Handoff Checklist (to be completed by Dev before QA)
- [ ] Story status moved to "Review" with latest test evidence attached.
- [ ] Logs/metrics screenshot or query output provided demonstrating successful run.
- [ ] SQL queries verifying schema and sample data attached in story comments.
- [ ] Runbook link updated and referenced.

## Implementation Log
| Date | Author | Notes |
| --- | --- | --- |

## QA Results
| Date | Reviewer | Outcome | Notes |
| --- | --- | --- | --- |

## File Tracker
- _To be populated by Dev agent upon implementation._

