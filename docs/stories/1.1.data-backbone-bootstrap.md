# Story 1.1: Data Backbone Bootstrap

## Status
Draft

## Story
**As a** TillLess platform operator,
**I want** the canonical product registry migration applied and the Checkers/Sixty60 ingestion loop populating fresh catalogue and price data via pg-boss,
**so that** downstream optimisation can rely on accurate, current data for the pilot geography.

## Acceptance Criteria
1. Supabase Postgres (staging and production) reflects the Phase 1 CPR migration `20241007_0001_cpr_phase1.sql`, with new columns (`price_mode`, `price_per_uom`, loyalty/promo metadata, store enrichment) verified through SQL introspection queries.
2. Checkers/Sixty60 scraper worker (TypeScript + Playwright) triggered by Temporalite publishes payloads to pg-boss and persists to CPR tables on manual and scheduled runs.
3. Successful ingestion writes populate `retailer_item` and `retailer_item_price` with at least 500 active SKUs covering staple categories (milk, bread, rice, oil, detergents, nappies, chicken) for a Johannesburg store.
4. `content_hash` prevents duplicate writes on consecutive runs; re-running the worker without catalogue changes results in no more than 5% new rows.
5. Workflow metrics/logs (duration, SKU count, success/failure) are visible in Temporalite UI and forwarded to the central log sink, with an alert configured for two consecutive failures.
6. Runbook entry documents how to trigger the Checkers ingestion (Temporal CLI and manual pnpm script) and verify data freshness in Postgres.

## Tasks / Subtasks
- [ ] Apply CPR Phase 1 migration to Supabase Postgres (AC: 1)
  - [ ] Execute migration `20241007_0001_cpr_phase1.sql` against staging, then production after validation, ensuring new CPR columns and constraints exist (AC: 1) [Source: canonical-product-registry.md#5-migration-outline-postgresql]
  - [ ] Capture verification queries (e.g., checking `price_mode`, promo fields) and append them to `db/README.md` for future reference (AC: 1, 6) [Source: canonical-product-registry.md#3-field-reference-delta-focused]
- [ ] Implement Checkers/Sixty60 scraper worker (AC: 2, 3, 4)
  - [ ] Build TypeScript Playwright worker following retailer playbook auth, cadence, and output contract requirements, including loyalty toggles and content hash generation (AC: 2, 4) [Source: retailer-scraping-playbook.md#21-checkers--sixty60]
  - [ ] Place worker implementation under `packages/ingestion-workers/checkers/` with shared helpers in `packages/ingestion-shared/` to match the source tree. (AC: 2) [Source: docs/architecture/source-tree.md#source-tree--module-layout]
  - [ ] Package worker for container execution and share types through a common package for Temporal activities (AC: 2) [Source: architecture/04-4-component-breakdown.md#41-data-ingestion-layer]
- [ ] Configure pg-boss queue and NestJS consumer (AC: 2, 3, 4)
  - [ ] Initialise `pg-boss` against Supabase Postgres, ensuring credentials and schema align with queue design (AC: 2) [Source: architecture/04-4-component-breakdown.md#41-data-ingestion-layer]
  - [ ] Map payloads into CPR tables via Prisma, handling promo metadata, per-weight pricing, and batching inserts (AC: 2, 3) [Source: architecture/04-4-component-breakdown.md#43-canonical-product-registry-cpr]
  - [ ] Keep consumer code inside `packages/queue-consumers/` with shared TypeScript contracts imported from `packages/types/ingestion.ts`. (AC: 2, 3) [Source: docs/architecture/source-tree.md#source-tree--module-layout]
  - [ ] Add automated tests covering payload validation and duplicate suppression leveraging the `content_hash` (AC: 4) [Source: architecture/06-6-technology-stack.md#technology-stack]
- [ ] Orchestrate Temporalite workflow execution (AC: 2, 4, 5)
  - [ ] Define Temporal workflow with retry/backoff policies and schedule cadence per ingestion grid, plus GitHub Actions cron fallback (AC: 2, 5) [Source: architecture/04-4-component-breakdown.md#41-data-ingestion-layer]
  - [ ] Validate consecutive workflow runs demonstrate idempotent behaviour (≤5% delta) and log metrics for duration/SKU counts (AC: 4, 5) [Source: architecture/09-9-monitoring-alerting.md#9-monitoring--alerting]
- [ ] Seed initial dataset and quality-check outputs (AC: 3, 4)
  - [ ] Execute workflow to populate ≥500 staple SKUs for a Johannesburg store and spot-check 10 items against live site for accuracy (AC: 3) [Source: retailer-scraping-playbook.md#3-scheduling-grid-ops-ready]
  - [ ] Confirm duplicate suppression and `content_hash` efficacy via SQL/Prisma queries (AC: 4) [Source: canonical-product-registry.md#4-matching--normalisation-rules-addenda]
- [ ] Establish observability and runbook documentation (AC: 5, 6)
  - [ ] Surface ingestion metrics/logs through Temporalite UI and forward structured events to Grafana/Logtail with failure alerts (AC: 5) [Source: architecture/04-4-component-breakdown.md#48-observability--ops]
  - [ ] Update `docs/ops/` runbook with trigger steps, verification queries, and troubleshooting guidance (AC: 6) [Source: architecture/04-4-component-breakdown.md#48-observability--ops]

## Dev Notes
**Previous Story Insights**
- No prior stories in Epic 1; this work seeds the data backbone for subsequent ingestion and optimisation efforts.

**Data Models & Storage**
- CPR schema extends product, store, and price tables with loyalty/promo metadata, per-weight pricing, and availability enums that migrations must apply before ingestion (e.g., `price_mode`, `price_per_uom`, promo columns) [Source: canonical-product-registry.md#3-field-reference-delta-focused]
- `retailer_item` and `retailer_item_price` store time-series snapshots tied to content hashes; ingestion must respect constraints to avoid redundant rows [Source: canonical-product-registry.md#2-entity-relationship-summary]

**Queue & Orchestration**
- Scraper workers publish structured payloads (`retailer`, `store_id`, `retailer_sku`, promo fields, etc.) to `pg-boss`, enabling retries and scheduled jobs within Supabase Postgres [Source: architecture/04-4-component-breakdown.md#41-data-ingestion-layer]
- Temporalite workflows (TypeScript) coordinate cadence, retries, and fallback cron triggers; ensure worker containers register activities accordingly [Source: architecture/04-4-component-breakdown.md#41-data-ingestion-layer]

**Scraper Implementation**
- Checkers/Sixty60 requires auth token rotation, loyalty toggles, category seeding, and availability mapping; throttle requests and honour concurrency caps while capturing promo badges [Source: retailer-scraping-playbook.md#21-checkers--sixty60]
- Shared scraping principles mandate delta detection (`content_hash`), structured logging, and raw field preservation for auditing [Source: retailer-scraping-playbook.md#1-shared-principles]

**Normalisation & Matching**
- NestJS matching service should reuse shared TypeScript utilities for name normalisation, pack parsing, and manual review queueing of low-confidence matches [Source: architecture/04-4-component-breakdown.md#42-normalisation--matching]
- Promo and per-weight data must map into CPR columns so optimisation engine can compute accurate unit prices later [Source: architecture/04-4-component-breakdown.md#43-canonical-product-registry-cpr]

**Observability & Runbooks**
- Capture ingestion metrics (SKU counts, latency, failure rates) with alerts for >2 consecutive failures; logs route to Grafana/Logtail per monitoring strategy [Source: architecture/09-9-monitoring-alerting.md#9-monitoring--alerting]
- Maintain Temporalite and ingestion runbooks in `docs/ops/` capturing triggers, verification steps, and troubleshooting escalation paths [Source: architecture/04-4-component-breakdown.md#48-observability--ops]

**Security & Secrets**
- Store retailer credentials (Sixty60 tokens) in secrets management (Supabase secrets or equivalent) and avoid logging sensitive data during ingestion [Source: retailer-scraping-playbook.md#1-shared-principles]

**Project Structure Notes**
- Follow `docs/architecture/source-tree.md` for folder placement (e.g., workers under `packages/ingestion-workers/` and shared utilities in `packages/ingestion-shared/`), adding new retailer-specific folders as needed.

### Testing
- Implement Jest-based tests for pg-boss consumer mapping and Temporal workflows to align with TypeScript stack expectations [Source: architecture/06-6-technology-stack.md#technology-stack]
- Time-box workflow runs to remain within ingestion SLA (<10 minutes for 500 SKUs) and validate duplicate suppression through automated assertions [Source: architecture/07-7-non-functional-considerations.md#7-non-functional-considerations]
- Key scenarios per testing strategy: migration verification (schema columns present), ingestion idempotency (duplicate suppression), queue consumer mapping (promo/loyalty fields), workflow retry behaviour, and observability hook assertions. [Source: docs/architecture/testing-strategy.md#testing-strategy]

## Change Log
| Date | Version | Description | Author |
| --- | --- | --- | --- |
| 2025-09-28 | v0.1 | Initial draft prepared by Scrum Master | Bob |

## Dev Agent Record
### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results
| Date | Reviewer | Outcome | Notes |
| --- | --- | --- | --- |
